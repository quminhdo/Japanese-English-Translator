# Model configuration
ENCODER_CELL_TYPE = 'norm_lstm'
ENCODER_LAYERS = 2
ENCODER_BI_LAYERS = 1
DECODER_CELL_TYPE = 'norm_lstm'
DECODER_LAYERS = 2
EMBEDDING_DIM = 512
HIDDEN_SIZE = 512
ATTENTION_SIZE = 512
USE_TRAINED_EMBEDDING = False
PASS_STATE = True
COMMON_FINAL_STATE = False
ATTENTION = 'bahdanau'
DROPOUT = 0.2
MAX_GRAD_NORM = 5.0
TIME_MAJOR = False
LAYER_NORM = False
NORMALIZE = False
DECODER = 'baseline'
APPLY_ATTENTION_TO_ALL = False
CONCATENATE = True
STDDEV = 1.0

# Training configuration
NUM_SAMPLED = 0
BATCH_SIZE = 100
LEARNING_RATE_0 = 0.004
LEARNING_RATE_1 = 0.002
LEARNING_RATE_2 = 0.001
LEARNING_RATE_3 = 0.0005
LEARNING_RATE_4 = 0.00025
LEARNING_RATE_5 = 0.0001
LEARNING_RATE_6 = 0.00005
DECAY_STEPS = 17000
DECAY_RATE = 0.5
BUFFER_SIZE = 4000000

# Inference configuration
MAX_LENGTH = 50
BEAM_WIDTH = 10
LENGTH_PENALTY = 1.0

# Data processing configuration
RAW_DIR = 'partitioned'
PROC_DIR = 'processed'
INFER_OUTPUT = 'infer.log'
CPT_DIR = 'checkpoints'
LANG_DICT = {'ja':'Japanese', 'en':'English'}
WORD_NUM = 0
DIV_RATE = 0.1

JA_VOCAB_SIZE = 46088
EN_VOCAB_SIZE = 35586
